{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKbWLdo23uVcPqmPpBF+74",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshhj/nlp-lab/blob/main/NLP_Prac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 - morphological"
      ],
      "metadata": {
        "id": "P2j-8U1MoaYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdTt4fYdoXCB",
        "outputId": "dfeda340-8036-4c6d-875d-0a72e96d8d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Token          UPOS      XPOS      Morphological Features\n",
            "------------------------------------------------------------\n",
            "All            DET       PDT       \n",
            "our            PRON      PRP$      Number=Plur|Person=1|Poss=Yes|PronType=Prs\n",
            "dreams         NOUN      NNS       Number=Plur\n",
            "can            AUX       MD        VerbForm=Fin\n",
            "come           VERB      VB        VerbForm=Inf\n",
            "true           ADJ       JJ        Degree=Pos\n",
            "—              PUNCT     :         \n",
            "if             SCONJ     IN        \n",
            "we             PRON      PRP       Case=Nom|Number=Plur|Person=1|PronType=Prs\n",
            "have           VERB      VBP       Mood=Ind|Tense=Pres|VerbForm=Fin\n",
            "the            DET       DT        Definite=Def|PronType=Art\n",
            "courage        NOUN      NN        Number=Sing\n",
            "to             PART      TO        \n",
            "pursue         VERB      VB        VerbForm=Inf\n",
            "them           PRON      PRP       Case=Acc|Number=Plur|Person=3|PronType=Prs\n",
            ".              PUNCT     .         PunctType=Peri\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"All our dreams can come true — if we have the courage to pursue them.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(f\"{'Token':<15}{'UPOS':<10}{'XPOS':<10}{'Morphological Features'}\")\n",
        "print(\"-\" * 60)\n",
        "for token in doc:\n",
        "  print(f\"{token.text:<15}{token.pos_:<10}{token.tag_:<10}{token.morph}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"they jump over brown tree\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# print(f\"{'Token':<15}{'UPOS':<10}{'XPOS':<10}{'Morphological Features'}\")\n",
        "print(\"Token\\t UPOS\\t XPOS\\t Morphological Features\\t\")\n",
        "print(\"-\" * 60)\n",
        "for token in doc:\n",
        "  # print(f\"{token.text:<15}{token.pos_:<10}{token.tag_:<10}{token.morph}\")\n",
        "  print(token.text, \"\\t\", token.pos_, \"\\t\", token.tag_, \"\\t\", token.morph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhyM4WJCouEI",
        "outputId": "b6942458-5789-411d-c3bd-efe3f39457ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token\t UPOS\t XPOS\t Morphological Features\t\n",
            "------------------------------------------------------------\n",
            "they \t PRON \t PRP \t Case=Nom|Number=Plur|Person=3|PronType=Prs\n",
            "jump \t VERB \t VBP \t Tense=Pres|VerbForm=Fin\n",
            "over \t ADP \t IN \t \n",
            "brown \t ADJ \t JJ \t Degree=Pos\n",
            "tree \t NOUN \t NN \t Number=Sing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EX 11 - 12 Chunkers"
      ],
      "metadata": {
        "id": "Ay4htf_lsW2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the English NLP pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = input(\"Enter a sentence: \")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Noun Phrases (Chunking) -->ex11 noun chunking\n",
        "print(\"\\nNoun Phrases:\")\n",
        "for chunk in doc.noun_chunks:\n",
        "  print(f\"- {chunk.text}\")\n",
        "\n",
        "# Named Entities\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "  print(f\"- {ent.text} ({ent.label_})\")\n",
        "\n",
        "# Part-of-Speech Tags (for each word)\n",
        "print(\"\\nPart-of-Speech Tags:\")\n",
        "for token in doc:\n",
        "  print(f\"- {token.text}: {token.pos_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78tJkAoPouuw",
        "outputId": "70d8ec24-074e-4dc9-d0c5-e12b62fa93e7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: ChatGPT is an amazing AI model.\n",
            "\n",
            "Noun Phrases:\n",
            "- ChatGPT\n",
            "- an amazing AI model\n",
            "\n",
            "Named Entities:\n",
            "- ChatGPT (ORG)\n",
            "\n",
            "Part-of-Speech Tags:\n",
            "- ChatGPT: PROPN\n",
            "- is: AUX\n",
            "- an: DET\n",
            "- amazing: ADJ\n",
            "- AI: PROPN\n",
            "- model: NOUN\n",
            "- .: PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the English NLP pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def extract_noun_phrases(text):\n",
        "  doc = nlp(text)\n",
        "  return [chunk.text for chunk in doc.noun_chunks]\n",
        "# Get input from user\n",
        "text = input(\"Enter a sentence: \")\n",
        "print(\"Noun Phrases:\")\n",
        "for np in extract_noun_phrases(text):\n",
        "  print(f\"- {np}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqg-D7M2sM1j",
        "outputId": "54409c41-34ac-430b-dbcb-e0587c03c123"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: lazy fox jumps over the brown tree\n",
            "Noun Phrases:\n",
            "- lazy fox\n",
            "- the brown tree\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ex 10 - pos tagger"
      ],
      "metadata": {
        "id": "a74Y5sgutclT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "text = \"ChatGPT is an amazing AI model.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlHA1V0js3aQ",
        "outputId": "b7dc40aa-8b44-4796-c7dc-448a318b9a26"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ChatGPT', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('amazing', 'JJ'), ('AI', 'NNP'), ('model', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ex 13 - ner"
      ],
      "metadata": {
        "id": "4QnZ0B8rwwVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the English NLP pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = input(\"Enter a sentence: \")\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Named Entities:\")\n",
        "for entity in doc.ents:\n",
        "  print(f\"- {entity.text} ({entity.label_})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1HBzVGUwttn",
        "outputId": "b4f6bd26-1da7-431e-95fa-ec1f4dc06caf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: apple was found by Steve in new jersey in 1667\n",
            "Named Entities:\n",
            "- apple (ORG)\n",
            "- Steve (PERSON)\n",
            "- new jersey (GPE)\n",
            "- 1667 (DATE)\n"
          ]
        }
      ]
    }
  ]
}