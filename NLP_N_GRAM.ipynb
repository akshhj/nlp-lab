{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCtO97WHgqu8JB/slCIjjb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshhj/nlp-lab/blob/main/NLP_N_GRAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4lnAdRBU0LB",
        "outputId": "555d3f41-760e-46bd-9b5b-e5182e9559ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word or sentence for prediction: united\n",
            "\n",
            "Bigram Model (Brown Corpus) Predictions:\n",
            "[('the', 5), ('</s>', 4), ('a', 3)]\n",
            "\n",
            "Trigram Model (WSJ Corpus) Predictions:\n",
            "[('</s>', 4), ('the', 3), ('investors', 2)]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "\n",
        "class NGramModel:\n",
        "    def __init__(self, n):\n",
        "        self.n = n  # Order of n-gram (2 for bigram, 3 for trigram)\n",
        "        self.ngram_counts = defaultdict(Counter)\n",
        "        self.context_counts = Counter()\n",
        "        self.vocab = set()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        tokens = text.split()\n",
        "        return [\"<s>\"] * (self.n - 1) + tokens\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for sentence in corpus:\n",
        "            tokens = self.preprocess(sentence) + [\"</s>\"]\n",
        "            self.vocab.update(tokens)\n",
        "            for i in range(len(tokens) - self.n + 1):\n",
        "                context = tuple(tokens[i:i + self.n - 1])\n",
        "                word = tokens[i + self.n - 1]\n",
        "                self.ngram_counts[context][word] += 1\n",
        "                self.context_counts[context] += 1\n",
        "\n",
        "    def predict(self, context, top_k=3):\n",
        "        context_tokens = self.preprocess(context)[-self.n + 1:]\n",
        "        context = tuple(context_tokens)\n",
        "\n",
        "        # If input is a single word, use <s> padding\n",
        "        if len(context) == 1:\n",
        "            context = tuple([\"<s>\"] * (self.n - 1) + list(context))[-self.n + 1:]\n",
        "\n",
        "        # Check if context exists\n",
        "        if context in self.ngram_counts:\n",
        "            candidates = self.ngram_counts[context]\n",
        "            total_count = sum(candidates.values())\n",
        "            predictions = [(word, count / total_count) for word, count in candidates.items()]\n",
        "            return sorted(predictions, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "        # If context is not found, return the most frequent words overall\n",
        "        most_common = Counter()\n",
        "        for counts in self.ngram_counts.values():\n",
        "            most_common.update(counts)\n",
        "        return most_common.most_common(top_k)\n",
        "\n",
        "# Expanded Sample Brown and WSJ Corpus\n",
        "brown_corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A journey of a thousand miles begins with a single step.\",\n",
        "    \"The dog barked at the stranger.\",\n",
        "    \"She sells sea shells by the seashore.\"\n",
        "]\n",
        "\n",
        "wsj_corpus = [\n",
        "    \"Stocks rose on Monday as investors showed optimism.\",\n",
        "    \"The Federal Reserve signaled a potential rate cut.\",\n",
        "    \"The market responded positively to the news.\",\n",
        "    \"Investors reacted strongly to economic reports.\"\n",
        "]\n",
        "\n",
        "# Train Bigram Model on Brown Corpus\n",
        "bigram_model_brown = NGramModel(2)\n",
        "bigram_model_brown.train(brown_corpus)\n",
        "\n",
        "# Train Trigram Model on WSJ Corpus\n",
        "trigram_model_wsj = NGramModel(3)\n",
        "trigram_model_wsj.train(wsj_corpus)\n",
        "\n",
        "# Get user input\n",
        "user_sentence = input(\"Enter a word or sentence for prediction: \")\n",
        "\n",
        "# Predict Next Word for User Input\n",
        "print(\"\\nBigram Model (Brown Corpus) Predictions:\")\n",
        "print(bigram_model_brown.predict(user_sentence))\n",
        "\n",
        "print(\"\\nTrigram Model (WSJ Corpus) Predictions:\")\n",
        "print(trigram_model_wsj.predict(user_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown, treebank\n",
        "from collections import Counter\n",
        "\n",
        "# Download required data\n",
        "nltk.download('brown')\n",
        "nltk.download('treebank')\n",
        "\n",
        "# Preprocess the corpus\n",
        "def preprocess_corpus(corpus):\n",
        "    \"\"\"Convert sentences into lowercase words.\"\"\"\n",
        "    return [[word.lower() for word in sent] for sent in corpus]\n",
        "\n",
        "# Load and preprocess Brown and WSJ corpora\n",
        "brown_sentences = preprocess_corpus(brown.sents())\n",
        "wsj_sentences = preprocess_corpus(treebank.sents())\n",
        "\n",
        "# Function to build n-gram models\n",
        "def build_ngram_model(sentences, n=2):\n",
        "    \"\"\"Create an n-gram model with frequency counts.\"\"\"\n",
        "    ngram_counts = Counter()\n",
        "    context_counts = Counter()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = ['<s>'] * (n - 1) + sentence + ['</s>']\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            ngram = tuple(sentence[i:i + n])\n",
        "            context = tuple(sentence[i:i + n - 1])\n",
        "            ngram_counts[ngram] += 1\n",
        "            context_counts[context] += 1\n",
        "\n",
        "    return ngram_counts, context_counts\n",
        "\n",
        "# Build bigram and trigram models for Brown and WSJ corpus\n",
        "bigram_counts_brown, bigram_contexts_brown = build_ngram_model(brown_sentences, n=2)\n",
        "trigram_counts_brown, trigram_contexts_brown = build_ngram_model(brown_sentences, n=3)\n",
        "\n",
        "bigram_counts_wsj, bigram_contexts_wsj = build_ngram_model(wsj_sentences, n=2)\n",
        "trigram_counts_wsj, trigram_contexts_wsj = build_ngram_model(wsj_sentences, n=3)\n",
        "\n",
        "# Function for next word prediction\n",
        "def next_word_prediction(ngram_counts, context_counts, context, top_n=3):\n",
        "    \"\"\"Predict the next word given a context, returning top candidates with probabilities.\"\"\"\n",
        "    context = tuple(context)\n",
        "    candidates = {ngram[-1]: count for ngram, count in ngram_counts.items() if ngram[:-1] == context}\n",
        "\n",
        "    if not candidates:\n",
        "        return []\n",
        "\n",
        "    total_count = sum(candidates.values())\n",
        "    sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [(word, count / total_count) for word, count in sorted_candidates[:top_n]]\n",
        "\n",
        "# Get user input\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter a word or phrase (or type 'exit' to quit): \").lower().strip()\n",
        "    if user_input == \"exit\":\n",
        "        break\n",
        "\n",
        "    words = user_input.split()\n",
        "\n",
        "    # Predict using the bigram model\n",
        "    if len(words) >= 1:\n",
        "        bigram_context = (words[-1],)\n",
        "\n",
        "        bigram_predictions_brown = next_word_prediction(bigram_counts_brown, bigram_contexts_brown, bigram_context)\n",
        "        bigram_predictions_wsj = next_word_prediction(bigram_counts_wsj, bigram_contexts_wsj, bigram_context)\n",
        "\n",
        "        print(\"\\nBigram Model Predictions:\")\n",
        "        print(\"Brown Corpus:\")\n",
        "        for word, prob in bigram_predictions_brown:\n",
        "            print(f\"{word}: {prob:.4f}\")\n",
        "\n",
        "        print(\"\\nWSJ Corpus:\")\n",
        "        for word, prob in bigram_predictions_wsj:\n",
        "            print(f\"{word}: {prob:.4f}\")\n",
        "\n",
        "    # Predict using the trigram model (if user input has at least 2 words)\n",
        "    if len(words) >= 2:\n",
        "        trigram_context = (words[-2], words[-1])\n",
        "\n",
        "        trigram_predictions_brown = next_word_prediction(trigram_counts_brown, trigram_contexts_brown, trigram_context)\n",
        "        trigram_predictions_wsj = next_word_prediction(trigram_counts_wsj, trigram_contexts_wsj, trigram_context)\n",
        "\n",
        "        print(\"\\nTrigram Model Predictions:\")\n",
        "        print(\"Brown Corpus:\")\n",
        "        for word, prob in trigram_predictions_brown:\n",
        "            print(f\"{word}: {prob:.4f}\")\n",
        "\n",
        "        print(\"\\nWSJ Corpus:\")\n",
        "        for word, prob in trigram_predictions_wsj:\n",
        "            print(f\"{word}: {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_9zNeeoU1OL",
        "outputId": "2c8f359e-1042-4141-c0da-8c415063556d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter a word or phrase (or type 'exit' to quit): the united\n",
            "\n",
            "Bigram Model Predictions:\n",
            "Brown Corpus:\n",
            "states: 0.8133\n",
            "nations: 0.1017\n",
            "states': 0.0083\n",
            "\n",
            "WSJ Corpus:\n",
            "states: 0.2632\n",
            "illuminating: 0.1579\n",
            "kingdom: 0.1579\n",
            "\n",
            "Trigram Model Predictions:\n",
            "Brown Corpus:\n",
            "states: 0.8550\n",
            "nations: 0.1069\n",
            "states': 0.0102\n",
            "\n",
            "WSJ Corpus:\n",
            "states: 0.6250\n",
            "steelworkers: 0.1250\n",
            "nations: 0.1250\n",
            "\n",
            "Enter a word or phrase (or type 'exit' to quit): family\n",
            "\n",
            "Bigram Model Predictions:\n",
            "Brown Corpus:\n",
            ".: 0.1118\n",
            ",: 0.0816\n",
            "of: 0.0574\n",
            "\n",
            "WSJ Corpus:\n",
            ",: 0.1667\n",
            "members: 0.0833\n",
            "were: 0.0417\n",
            "\n",
            "Enter a word or phrase (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YDmvf7GRVCJ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}